name: Data Fetching
on: 
  schedule:
    - cron: "0 0 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Setup Python environment
        uses: actions/setup-python@v5
        with:
            python-version: '3.12'
            architecture: 'x64'
            cache: 'pip'
            cache-dependency-path: _crawling/requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ./_crawling/requirements.txt

      - name: Set up chrome driver
        uses: nanasess/setup-chromedriver@v2

      - name: Set up chrome driver
        run: |
          export DISPLAY=:99
          chromedriver --url-base=/wd/hub &
          sudo Xvfb -ac :99 -screen 0 1280x1024x24 > /dev/null 2>&1 & # optional

      - name: Crawl
        run: |
          set -o xtrace
          python ./_crawling/main.py

      - name: Upload crawling outputs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: Crawled Files
          retention-days: 7
          path: |
            fleet.json
            fleet.min.json

      - name: Create "build" directory
        run: mkdir -p build

      - name: Copy output to "build" directory
        run: |
          cp \
          fleet.json \
          fleet.min.json \
          build/

      - name: Update resources
        uses: JamesIves/github-pages-deploy-action@v4
        with:
            folder: build
            commit-message: Update resources
            branch: data
            clean: false
